{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Segment Anything Model 2 (SAM 2)**\n",
        "![SAM2](https://github.com/ibaiGorordo/ONNX-SAM2-Segment-Anything/raw/main/doc/img/sam2_annotation.gif)"
      ],
      "metadata": {
        "id": "qlsCAu5JBIcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation !!Requires GPU runtime!!"
      ],
      "metadata": {
        "id": "TVR_BlMZD4XJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sBSQnXVFAiwg",
        "outputId": "33994785-24fb-48f0-9fa6-7c554f19b21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'segment-anything-2' already exists and is not an empty directory.\n",
            "/content/segment-anything-2\n",
            "Obtaining file:///content/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (0.18.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (4.66.5)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (1.3.2)\n",
            "Requirement already satisfied: iopath>=0.1.10 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (0.1.10)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (9.4.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (2.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.1->SAM-2==1.0) (12.6.20)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.1->SAM-2==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.1->SAM-2==1.0) (1.3.0)\n",
            "Building wheels for collected packages: SAM-2\n",
            "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=SAM_2-1.0-0.editable-cp310-cp310-linux_x86_64.whl size=12322 sha256=5a4c2a3993372d4f3cb506a6cf1b922e3f20621e2ebf44804bb5868e05090689\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7zzz370q/wheels/7d/af/fe/c05425a1fdc391329545b53111d5cabdfc241ee07cab053945\n",
            "Successfully built SAM-2\n",
            "Installing collected packages: SAM-2\n",
            "  Attempting uninstall: SAM-2\n",
            "    Found existing installation: SAM-2 1.0\n",
            "    Uninstalling SAM-2-1.0:\n",
            "      Successfully uninstalled SAM-2-1.0\n",
            "Successfully installed SAM-2-1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sam2",
                  "sam2_configs"
                ]
              },
              "id": "aa560bb501504030ab74cbd178f01cd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.16.2)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20240818)\n",
            "Requirement already satisfied: onnxsim in /usr/local/lib/python3.10/dist-packages (0.4.36)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnxsim) (13.7.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim) (2.16.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd /content/segment-anything-2\n",
        "!pip3 install -e .\n",
        "!pip3 install onnx onnxscript onnxsim onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/checkpoints\n",
        "!./download_ckpts.sh"
      ],
      "metadata": {
        "id": "Vtld9UUcAxH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da05c017-291c-4b56-88c8-aec6f472f338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2/checkpoints\n",
            "Downloading sam2_hiera_tiny.pt checkpoint...\n",
            "--2024-08-18 06:36:38--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.35, 18.165.83.91, 18.165.83.44, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155906050 (149M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2_hiera_tiny.pt’\n",
            "\n",
            "sam2_hiera_tiny.pt  100%[===================>] 148.68M   140MB/s    in 1.1s    \n",
            "\n",
            "2024-08-18 06:36:39 (140 MB/s) - ‘sam2_hiera_tiny.pt’ saved [155906050/155906050]\n",
            "\n",
            "Downloading sam2_hiera_small.pt checkpoint...\n",
            "--2024-08-18 06:36:39--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.35, 18.165.83.91, 18.165.83.44, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 184309650 (176M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2_hiera_small.pt’\n",
            "\n",
            "sam2_hiera_small.pt 100%[===================>] 175.77M   144MB/s    in 1.2s    \n",
            "\n",
            "2024-08-18 06:36:40 (144 MB/s) - ‘sam2_hiera_small.pt’ saved [184309650/184309650]\n",
            "\n",
            "Downloading sam2_hiera_base_plus.pt checkpoint...\n",
            "--2024-08-18 06:36:40--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.35, 18.165.83.91, 18.165.83.44, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 323493298 (309M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2_hiera_base_plus.pt’\n",
            "\n",
            "sam2_hiera_base_plu 100%[===================>] 308.51M   176MB/s    in 1.8s    \n",
            "\n",
            "2024-08-18 06:36:42 (176 MB/s) - ‘sam2_hiera_base_plus.pt’ saved [323493298/323493298]\n",
            "\n",
            "Downloading sam2_hiera_large.pt checkpoint...\n",
            "--2024-08-18 06:36:42--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.35, 18.165.83.91, 18.165.83.44, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 897952466 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2_hiera_large.pt’\n",
            "\n",
            "sam2_hiera_large.pt 100%[===================>] 856.35M   119MB/s    in 8.7s    \n",
            "\n",
            "2024-08-18 06:36:51 (97.9 MB/s) - ‘sam2_hiera_large.pt’ saved [897952466/897952466]\n",
            "\n",
            "All checkpoints are downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "from typing import Optional, Tuple, Any\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import trunc_normal_\n",
        "\n",
        "\n",
        "from sam2.modeling.sam2_base import SAM2Base\n",
        "\n",
        "class SAM2ImageEncoder(nn.Module):\n",
        "    def __init__(self, sam_model: SAM2Base) -> None:\n",
        "        super().__init__()\n",
        "        self.model = sam_model\n",
        "        self.image_encoder = sam_model.image_encoder\n",
        "        self.no_mem_embed = sam_model.no_mem_embed\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[Any, Any, Any]:\n",
        "        backbone_out = self.image_encoder(x)\n",
        "        backbone_out[\"backbone_fpn\"][0] = self.model.sam_mask_decoder.conv_s0(\n",
        "            backbone_out[\"backbone_fpn\"][0]\n",
        "        )\n",
        "        backbone_out[\"backbone_fpn\"][1] = self.model.sam_mask_decoder.conv_s1(\n",
        "            backbone_out[\"backbone_fpn\"][1]\n",
        "        )\n",
        "\n",
        "        feature_maps = backbone_out[\"backbone_fpn\"][-self.model.num_feature_levels:]\n",
        "        vision_pos_embeds = backbone_out[\"vision_pos_enc\"][-self.model.num_feature_levels:]\n",
        "\n",
        "        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]\n",
        "\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]\n",
        "        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]\n",
        "\n",
        "        vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n",
        "\n",
        "        feats = [feat.permute(1, 2, 0).reshape(1, -1, *feat_size)\n",
        "                 for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
        "\n",
        "        return feats[0], feats[1], feats[2]\n",
        "\n",
        "\n",
        "class SAM2ImageDecoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            sam_model: SAM2Base,\n",
        "            multimask_output: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.mask_decoder = sam_model.sam_mask_decoder\n",
        "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
        "        self.model = sam_model\n",
        "        self.multimask_output = multimask_output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "            self,\n",
        "            image_embed: torch.Tensor,\n",
        "            high_res_feats_0: torch.Tensor,\n",
        "            high_res_feats_1: torch.Tensor,\n",
        "            point_coords: torch.Tensor,\n",
        "            point_labels: torch.Tensor,\n",
        "            mask_input: torch.Tensor,\n",
        "            has_mask_input: torch.Tensor,\n",
        "            img_size: torch.Tensor\n",
        "    ):\n",
        "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
        "        self.sparse_embedding = sparse_embedding\n",
        "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
        "\n",
        "        high_res_feats = [high_res_feats_0, high_res_feats_1]\n",
        "        image_embed = image_embed\n",
        "\n",
        "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
        "            image_embeddings=image_embed,\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embedding,\n",
        "            dense_prompt_embeddings=dense_embedding,\n",
        "            repeat_image=False,\n",
        "            high_res_features=high_res_feats,\n",
        "        )\n",
        "\n",
        "        if self.multimask_output:\n",
        "            masks = masks[:, 1:, :, :]\n",
        "            iou_predictions = iou_predictions[:, 1:]\n",
        "        else:\n",
        "            masks, iou_predictions = self.mask_decoder._dynamic_multimask_via_stability(masks, iou_predictions)\n",
        "\n",
        "        masks = torch.clamp(masks, -32.0, 32.0)\n",
        "        masks = masks > 0.0\n",
        "        masks = masks.to(torch.float32)\n",
        "        masks = masks * 255.0\n",
        "\n",
        "        masks = F.interpolate(masks, (img_size[0], img_size[1]), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return masks, iou_predictions\n",
        "\n",
        "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        point_coords = point_coords + 0.5\n",
        "\n",
        "        padding_point = torch.zeros((point_coords.shape[0], 1, 2), device=point_coords.device)\n",
        "        padding_label = -torch.ones((point_labels.shape[0], 1), device=point_labels.device)\n",
        "        point_coords = torch.cat([point_coords, padding_point], dim=1)\n",
        "        point_labels = torch.cat([point_labels, padding_label], dim=1)\n",
        "\n",
        "        point_coords[:, :, 0] = point_coords[:, :, 0] / self.model.image_size\n",
        "        point_coords[:, :, 1] = point_coords[:, :, 1] / self.model.image_size\n",
        "\n",
        "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
        "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
        "\n",
        "        point_embedding = point_embedding * (point_labels != -1)\n",
        "        point_embedding = point_embedding + self.prompt_encoder.not_a_point_embed.weight * (\n",
        "                point_labels == -1\n",
        "        )\n",
        "\n",
        "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
        "            point_embedding = point_embedding + self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
        "\n",
        "        return point_embedding\n",
        "\n",
        "    def _embed_masks(self, input_mask: torch.Tensor, has_mask_input: torch.Tensor) -> torch.Tensor:\n",
        "        mask_embedding = has_mask_input * self.prompt_encoder.mask_downscaling(input_mask)\n",
        "        mask_embedding = mask_embedding + (\n",
        "                1 - has_mask_input\n",
        "        ) * self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
        "        return mask_embedding"
      ],
      "metadata": {
        "id": "xQAznoeDIyak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b897011f-28f9-48fc-d341-b0dff6c12d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select  model parameters"
      ],
      "metadata": {
        "id": "DGJ8EjKE7zd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'sam2_hiera_base_plus' #@param [\"sam2_hiera_tiny\", \"sam2_hiera_small\", \"sam2_hiera_large\", \"sam2_hiera_base_plus\"]\n",
        "# input_size = 768 #@param {type:\"slider\", min:160, max:4102, step:8}\n",
        "input_size = 1024 # Bad output if anything else (for now)\n",
        "multimask_output = False\n",
        "\n",
        "if model_type == \"sam2_hiera_tiny\":\n",
        "    model_cfg = \"sam2_hiera_t.yaml\"\n",
        "elif model_type == \"sam2_hiera_small\":\n",
        "    model_cfg = \"sam2_hiera_s.yaml\"\n",
        "elif model_type == \"sam2_hiera_base_plus\":\n",
        "    model_cfg = \"sam2_hiera_b+.yaml\"\n",
        "else:\n",
        "    model_cfg = \"sam2_hiera_l.yaml\"\n"
      ],
      "metadata": {
        "id": "K-Ll5Iwh7428"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Encoder"
      ],
      "metadata": {
        "id": "t46_lYeIy0ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/segment-anything-2/\n",
        "import torch\n",
        "from sam2.build_sam import build_sam2\n",
        "\n",
        "sam2_checkpoint = f\"checkpoints/{model_type}.pt\"\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
        "\n",
        "img=torch.randn(1, 3, input_size, input_size).cpu()\n",
        "\n",
        "sam2_encoder = SAM2ImageEncoder(sam2_model).cpu()\n",
        "high_res_feats_0, high_res_feats_1, image_embed = sam2_encoder(img)\n",
        "print(high_res_feats_0.shape)\n",
        "print(high_res_feats_1.shape)\n",
        "print(image_embed.shape)\n",
        "\n",
        "torch.onnx.export(sam2_encoder,\n",
        "                  img,\n",
        "                  f\"{model_type}_encoder.onnx\",\n",
        "                  export_params=True,\n",
        "                  opset_version=17,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['image'],\n",
        "                  output_names = ['high_res_feats_0', 'high_res_feats_1', 'image_embed']\n",
        "                )"
      ],
      "metadata": {
        "id": "IgHx4lbupej-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b44ead8-e488-4b4e-dd88-4ace505beda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 64, 128, 128])\n",
            "torch.Size([1, 256, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/segment-anything-2/sam2/modeling/backbones/utils.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_h > 0 or pad_w > 0:\n",
            "/content/segment-anything-2/sam2/modeling/backbones/utils.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if Hp > H or Wp > W:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Decoder"
      ],
      "metadata": {
        "id": "JX1N64Y6y2-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "\n",
        "\n",
        "sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
        "\n",
        "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
        "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
        "mask_input_size = [4 * x for x in embed_size]\n",
        "print(embed_dim, embed_size, mask_input_size)\n",
        "\n",
        "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
        "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
        "mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
        "has_mask_input = torch.tensor([1], dtype=torch.float)\n",
        "orig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\n",
        "\n",
        "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\n",
        "\n",
        "\n",
        "torch.onnx.export(sam2_decoder,\n",
        "                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\n",
        "                  f\"{model_type}_decoder.onnx\",\n",
        "                  export_params=True,\n",
        "                  opset_version=16,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size'],\n",
        "                  output_names = ['masks', 'iou_predictions'],\n",
        "                  dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\n",
        "                                  \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\n",
        "                                  \"mask_input\": {0: \"num_labels\"},\n",
        "                                  \"has_mask_input\": {0: \"num_labels\"}\n",
        "                  }\n",
        "                )\n"
      ],
      "metadata": {
        "id": "KKqrn0sHaQYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6e613d-a9d8-473a-b551-cabc40441082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n",
            "256 (64, 64) [256, 256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/segment-anything-2/sam2/modeling/sam/mask_decoder.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert image_embeddings.shape[0] == tokens.shape[0]\n",
            "/content/segment-anything-2/sam2/modeling/sam/mask_decoder.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  image_pe.size(0) == 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 16 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplify models"
      ],
      "metadata": {
        "id": "dMgwgcWO18hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "!onnxsim {model_type}_encoder.onnx {model_type}_encoder.onnx\n",
        "!onnxsim {model_type}_decoder.onnx {model_type}_decoder.onnx"
      ],
      "metadata": {
        "id": "w4nMB2XD1-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional, mount GDrive for faster model download (Copy it to your Google Drive and then download)"
      ],
      "metadata": {
        "id": "JxyJ9H5xjoTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "metadata": {
        "id": "J6ameAOEjm9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0156245f-66eb-4da1-fc9b-91ebdade489c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "!cp {model_type}_encoder.onnx '/content/gdrive/My Drive/'\n",
        "!cp {model_type}_decoder.onnx '/content/gdrive/My Drive/'"
      ],
      "metadata": {
        "id": "ZyBDSz5RjAb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece91850-d66c-4423-e73f-7836d31f5891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2ZM_igGlnEe",
        "outputId": "d9828273-dc90-4b66-c1d7-a39a0f71868b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "session = ort.InferenceSession(\"/content/segment-anything-2/sam2_hiera_base_plus_decoder.onnx\" )\n",
        "print( [ t.shape for t in session.get_inputs() ] )\n",
        "print( [ t.type for t in session.get_inputs() ] )\n",
        "print( [ t.name for t in session.get_inputs() ] )\n",
        "print( [ t.shape for t in session.get_outputs() ] )\n",
        "print( [ t.type for t in session.get_outputs() ] )\n",
        "print( [ t.name for t in session.get_outputs() ] )\n",
        "print('\\n\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwpqCttDlsOh",
        "outputId": "4eb99fe4-dcae-43be-8fe5-03afb55f1b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 256, 64, 64], [1, 32, 256, 256], [1, 64, 128, 128], ['num_labels', 'num_points', 2], ['num_labels', 'num_points'], ['num_labels', 1, 256, 256], ['num_labels'], [2]]\n",
            "['tensor(float)', 'tensor(float)', 'tensor(float)', 'tensor(float)', 'tensor(float)', 'tensor(float)', 'tensor(float)', 'tensor(int32)']\n",
            "['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size']\n",
            "[['Resizemasks_dim_0', 'Resizemasks_dim_1', 'Resizemasks_dim_2', 'Resizemasks_dim_3'], ['Resizemasks_dim_0', 'Whereiou_predictions_dim_1']]\n",
            "['tensor(float)', 'tensor(float)']\n",
            "['masks', 'iou_predictions']\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/segment-anything-2/sam2_hiera_base_plus_decoder.onnx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "D1VJtWbI6Svd",
        "outputId": "da601df8-5eee-4712-ad36-b52677838ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_269f8a05-a455-409f-822c-ea99f536a38f\", \"sam2_hiera_base_plus_decoder.onnx\", 16541001)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}